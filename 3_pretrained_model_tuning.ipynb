{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2ce74fb",
   "metadata": {},
   "source": [
    "# **Pneumonia Detection**\n",
    "**A machine learning project for detecting pneumonia from chest X-ray images. It includes data preprocessing, feature extraction, and performance evaluation to aid early diagnosis.**\n",
    "\n",
    "## **Deep Model Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3a5641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import ClassifierMixin\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.axes import Axes\n",
    "\n",
    "KFOLD = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec6251e",
   "metadata": {},
   "source": [
    "### **③ Learning Curve**\n",
    "Illustrates how the classifier's performance evolves as the volume of training data increases. The red line tracks the training score, whereas the green line represents the cross-validation (test) score. Shaded regions surrounding each line highlight the variability in these scores. A high training score paired with a low validation score may suggest overfitting, while low scores on both curves can indicate underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20fe6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "\n",
    "def plot_learning_curve(\n",
    "  estimator: ClassifierMixin,\n",
    "  scorer: callable,\n",
    "  X_train: np.ndarray,\n",
    "  y_train: np.ndarray,\n",
    "  ax: Axes,\n",
    "):\n",
    "  _, train_scores, test_scores, _, _ = learning_curve(\n",
    "    estimator, X_train, y_train, cv=KFOLD, scoring=scorer, return_times=True\n",
    "  )\n",
    "\n",
    "  train_sizes = np.linspace(0, 1.0, 5)\n",
    "\n",
    "  train_scores_mean = np.mean(train_scores, axis=1)\n",
    "  train_scores_std = np.std(train_scores, axis=1)\n",
    "  test_scores_mean = np.mean(test_scores, axis=1)\n",
    "  test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "  # Plot learning curve\n",
    "  ax.grid()\n",
    "  ax.set_xlim(0.0, 1.05)\n",
    "  ax.set_xlabel(\"Data Percentage\")\n",
    "  ax.set_ylim(0.0, 1.05)\n",
    "  ax.set_ylabel(\"F1 Score\")\n",
    "  ax.fill_between(\n",
    "    train_sizes,\n",
    "    train_scores_mean - train_scores_std,\n",
    "    train_scores_mean + train_scores_std,\n",
    "    alpha=0.1,\n",
    "    color=\"r\",\n",
    "  )\n",
    "  ax.fill_between(\n",
    "    train_sizes,\n",
    "    test_scores_mean - test_scores_std,\n",
    "    test_scores_mean + test_scores_std,\n",
    "    alpha=0.1,\n",
    "    color=\"g\",\n",
    "  )\n",
    "  ax.plot(train_sizes, train_scores_mean, \"o-\", color=\"r\", label=\"Training score\")\n",
    "  ax.plot(\n",
    "    train_sizes, test_scores_mean, \"o-\", color=\"g\", label=\"Cross-validation score\"\n",
    "  )\n",
    "  ax.legend(loc=\"best\")\n",
    "  plot_title = f\"{type(estimator).__name__} Learning Curve\"\n",
    "  ax.set_title(plot_title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf13eac4",
   "metadata": {},
   "source": [
    "### **③ Feature Importance**\n",
    "Illustrates the average permutation importance of each Local Binary Pattern (LBP) feature for the selected classifier, computed over 30 random shuffles. Taller bars denote features whose perturbation causes the greatest drop in accuracy, highlighting their stronger contribution to the model’s predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae23343",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "\n",
    "def plot_feature_importance(\n",
    "  estimator: ClassifierMixin,\n",
    "  scorer: callable,\n",
    "  X_test: np.ndarray,\n",
    "  y_test: np.ndarray,\n",
    "  ax: Axes,\n",
    "):\n",
    "  result = permutation_importance(\n",
    "    estimator, X_test, y_test, n_repeats=30, scoring=scorer, random_state=42\n",
    "  )\n",
    "  pd.DataFrame(result.importances_mean).plot.bar(\n",
    "    y=0,\n",
    "    ylabel=\"Mean Importance\",\n",
    "    use_index=True,\n",
    "    xlabel=\"LBP Feature\",\n",
    "    rot=45,\n",
    "    align=\"center\",\n",
    "    legend=False,\n",
    "    grid=True,\n",
    "    title=f\"{type(estimator).__name__} LBP Feature Importance\",\n",
    "    ax=ax,\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8279f06",
   "metadata": {},
   "source": [
    "### **③ Confusion Matrix**\n",
    "Presents the confusion matrix for the classifier, comparing the actual test labels with the predicted results. The varying shades of blue indicate the frequency of predictions, making it easy to identify areas where the model excels or needs improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a8ef5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(\n",
    "  estimator: ClassifierMixin, y_test: np.ndarray, y_guess: np.ndarray, ax: Axes\n",
    "):\n",
    "  disp = ConfusionMatrixDisplay(confusion_matrix(y_test, y_guess))\n",
    "  disp.plot(cmap=plt.cm.Blues, ax=ax)\n",
    "  disp.ax_.set_title(f\"{type(estimator).__name__} Confusion Matrix\")\n",
    "  disp.ax_.set_xlabel(\"Predicted Label\")\n",
    "  disp.ax_.set_ylabel(\"True Label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db97f540",
   "metadata": {},
   "source": [
    "### **③ AUC-ROC Curve**\n",
    "Displays the trade-off between the True Positive Rate (TPR) and the False Positive Rate (FPR) across different classification thresholds. The area under the ROC curve (AUC) quantifies the model's ability to distinguish between the classes (the closer the AUC is to 1, the better the performance). The gray dashed line represents a baseline where predictions are equivalent to random guessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7edac11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "def plot_aucroc_curve(\n",
    "  estimator: ClassifierMixin, X_test: np.ndarray, y_test: np.ndarray, ax: Axes\n",
    "):\n",
    "  y_pred_proba = estimator.predict_proba(X_test)[:, 1]\n",
    "  fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "  roc_auc = auc(fpr, tpr)\n",
    "  pd.DataFrame({\"fpr\": fpr, \"tpr\": tpr}).plot(\n",
    "    \"fpr\",\n",
    "    \"tpr\",\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    "    label=f\"AUC = {roc_auc:.3f}\",\n",
    "    xlim=[0.0, 1.05],\n",
    "    ylim=[0.0, 1.05],\n",
    "    grid=True,\n",
    "    title=f\"{type(estimator).__name__} AUC-ROC Curve\",\n",
    "    ax=ax,\n",
    "  )\n",
    "  ax.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\")\n",
    "  ax.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2cf96a",
   "metadata": {},
   "source": [
    "### **③ Precision-Recall Curve**\n",
    "Shows the trade-off between precision and recall as the decision threshold shifts. The curve's area, expressed as the Average Precision (AP) score, encapsulates the model's overall performance. A higher AP score suggests an excellent balance between precision and recall, a critical measure when dealing with imbalanced datasets. The gray dashed line serves as a baseline for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f0acca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "\n",
    "def plot_prcrcl_curve(\n",
    "  estimator: ClassifierMixin, X_test: np.ndarray, y_test: np.ndarray, ax: Axes\n",
    "):\n",
    "  y_pred_proba = estimator.predict_proba(X_test)[:, 1]\n",
    "  precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "  avg_precision = average_precision_score(y_test, y_pred_proba)\n",
    "  pd.DataFrame({\"recall\": recall, \"precision\": precision}).plot(\n",
    "    \"recall\",\n",
    "    \"precision\",\n",
    "    xlabel=\"Recall\",\n",
    "    ylabel=\"Precision\",\n",
    "    label=f\"AP = {avg_precision:.3f}\",\n",
    "    xlim=[0.0, 1.05],\n",
    "    ylim=[0.0, 1.05],\n",
    "    grid=True,\n",
    "    title=f\"{type(estimator).__name__} Precision-Recall Curve\",\n",
    "    ax=ax,\n",
    "  )\n",
    "  ax.plot([0, 1], [1, 0], color=\"gray\", linestyle=\"--\")\n",
    "  ax.legend(loc=\"lower left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f262e929",
   "metadata": {},
   "source": [
    "### **② Pretrained Model Finetuning**\n",
    "Utilizes GridSearchCV with stratified k-fold cross-validation to optimize hyperparameters based on the F1 score. Once the best estimator is selected, multiple plots are generated to diagnose model performance, including the learning curve, confusion matrix, AUC-ROC curve, and precision-recall curve. Finally, the performance metrics (F1, precision, and recall) on the test set is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4c383a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "def finetune_pretrained(\n",
    "  estimator: ClassifierMixin,\n",
    "  X_train: np.ndarray,\n",
    "  y_train: np.ndarray,\n",
    "  X_test: np.ndarray,\n",
    "  y_test: np.ndarray,\n",
    "  axes: np.ndarray,\n",
    ") -> list:\n",
    "  # cv = StratifiedKFold(n_splits=KFOLD, shuffle=True, random_state=42)\n",
    "  fitted_estimator = estimator.fit(X_train, y_train)\n",
    "  # plot_learning_curve(fitted_estimator, f1_scorer, X_train, y_train, axes[0])\n",
    "  y_guess = fitted_estimator.predict(X_test)\n",
    "  plot_confusion_matrix(fitted_estimator, y_test, y_guess, axes[0])\n",
    "  plot_aucroc_curve(fitted_estimator, X_test, y_test, ax=axes[1])\n",
    "  plot_prcrcl_curve(fitted_estimator, X_test, y_test, ax=axes[2])\n",
    "\n",
    "  return [\n",
    "    f1_score(y_test, y_guess),\n",
    "    precision_score(y_test, y_guess),\n",
    "    recall_score(y_test, y_guess),\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a03786e",
   "metadata": {},
   "source": [
    "### **① Models for Finetuning**\n",
    "Defines a collection of model recipes to fine tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3322bb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from _pretrained_model import (\n",
    "  ConvNeXTPretrainedClassifier,\n",
    "  EfficientNetPretrainedClassifier,\n",
    "  ResNetPretrainedClassifier,\n",
    "  SwinPretrainedClassifier,\n",
    "  ViTPretrainedClassifier,\n",
    ")\n",
    "\n",
    "models = [\n",
    "  ConvNeXTPretrainedClassifier(random_state=42, max_iter=10),\n",
    "  EfficientNetPretrainedClassifier(random_state=42, max_iter=10),\n",
    "  ResNetPretrainedClassifier(random_state=42, max_iter=10),\n",
    "  SwinPretrainedClassifier(random_state=42, max_iter=10),\n",
    "  ViTPretrainedClassifier(random_state=42, max_iter=10),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2483cd0",
   "metadata": {},
   "source": [
    "### **① Pneumonia Classification Pipeline**\n",
    "Loads a pneumonia dataset and splits the data into training and testing sets. Then, create a multi-panel figure to display several diagnostic plots for each classifier in the recipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c28bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from _data import load_pneumonia_2d\n",
    "\n",
    "fig, axs = plt.subplots(len(models), 3, figsize=(19.5, 6.5 * len(models)))\n",
    "train, test = load_pneumonia_2d()\n",
    "X_train, y_train = train[\"image\"], train[\"label\"]\n",
    "X_test, y_test = test[\"image\"], test[\"label\"]\n",
    "\n",
    "test_score = pd.DataFrame(columns=[\"F1\", \"Precision\", \"Recall\"])\n",
    "for index, model in enumerate(models):\n",
    "  print(type(model).__name__)\n",
    "  test_score.loc[type(model).__name__] = finetune_pretrained(\n",
    "    model, X_train, y_train, X_test, y_test, axs[index, :]\n",
    "  )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b38d283",
   "metadata": {},
   "source": [
    "### **④ Model Performance Comparison**\n",
    "Compares the performance of various classifier on the testing set with bar chart. Model names are displayed along the x-axis, while their corresponding scores appear on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece232e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(1, 3, figsize=(19.5, 6.5))\n",
    "for index, col in enumerate(test_score.columns):\n",
    "  ax = axs[index]\n",
    "  test_score.plot.bar(\n",
    "    y=col,\n",
    "    use_index=True,\n",
    "    ylim=[0.0, 1.0],\n",
    "    legend=False,\n",
    "    grid=True,\n",
    "    title=f\"Testing Set {col} Score by Model\",\n",
    "    ax=ax,\n",
    "  )\n",
    "  ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS6220",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
